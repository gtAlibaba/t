언더 샘플링? 오버 샘플링? (Under Sampling / Over Sampling)

샘플링은 데이터의 특성에 따라 언더 샘플링(Under Sampling)을 하거나 오버 샘플링(Over Sampling)을 하기도 합니다.
언더 샘플링은 모집단에서 기준에 따라 표본을 추출하는 기법이고, 오버샘플링은 모집단의 데이터를 늘리는 기법입니다.

앞서 특징을 추출한다고 했는데, 오버(Over)라는 말이 이상하게 느껴질 수도 있는데, 오버 샘플링은 데이터의 수가 적은 경우에 기존 데이터의 특징을 기반으로 
데이터 수를 늘려서 데이터셋을 만드는 기법입니다. 표본을 추출할 때 똑같은 데이터를 반복해서 추출하여 데이터를 늘리거나 통계량을 기반으로 임의의 데이터를 생성하여 
표본을 추출하여 표본의 수를 늘려주는 것입니다. 표본 데이터를 증식한다고 이해하시면 될 것 같습니다.

ㅁ 언더 샘플링은 랜덤추출법, 계통추출법, 층화추출법등의 기법을 사용하고, 오버샘플링은 ROSE, SMOTE등의 기법을 사용합니다.

 - 언더 샘플링 기법 (Under Sampling)
   : 랜덤 추출법, 계통 추출법,집락 추출법, 층화 추출법

1. 단순랜덤 추출법 (임의로 n개를 추출)
모집단의 데이터에서 랜덤으로 데이터를 추출하는 방법입니다. 데이터가 굉장히 많은 경우에는 랜덤으로 추출하더라도 표본이 모집단의 특징을 대변할 수 있지만 
데이터가 적은 경우는 편향된 표본이 추출될 가능성도 존재합니다.

2. 계통 추출법 ( 매 K번째 항목을 n개 추출)
모집단의 데이터를 일렬로 나열했을 때, k번째 항목을 n개 수출하는 방법입니다. 예를 들어 100개의 데이터 중 10개의 표본을 뽑는다고 했을 떄 
10,20,30,40,50,60,70,80,90,100 으로 10번째 항목을 10개 추출하는 방식입니다.

3. 집락추출법 (군집을 구분하고 군집별로 단순랜덤 추출)
집락추출법은 모집단에서 집단을 일차적으로 추출하고, 일차로 만들어진 집단을 군집이라고 합니다. 만들어진 군집별로 단순랜덤 추출을 하는 방식이 집락추출법입니다.

4. 층화 추출법(모집단에서 각 계층을 고루 대표할 수 있도록 표본을 추출)
층화 추출법은 모집단의 그룹을 나눌 수 있는 경우 각 그룹별로 표본을 추출하는 방식입니다. 예를 들어 100,000명의 나이/지역 정보가 들어있는
데이터에서 지역별 나이를 분석하고 싶은 경우, 표본을 지역별로 추출하는 것입니다.


 - 오버 샘플링 기법 (Over Sampling)
  : SMOTE, ADASYN, ROS(ROSE)
오버샘플링(Over Sampling) 기법은 앞서 설명드린대로 데이터의 양을 증식시키는 방법입니다. 통상 학습데이터가 부족한 경우 오버샘플링을 수행합니다.
대표적인 방법으로는 SMOTE와 ROSE가 있습니다.
1. SMOTE (synthetic minority oversampling technique)
오버샘플링 - SMOTE

1) 데이터 중 소수 데이터가 존재하는 집단을 선정한다.
2) 선정된 집단에서 하나의 표본을 뽑아 K-nearest neighbor중 하나를 선택한다.
3) 선택된 표본과 쌍을 잇는 선을 그리고 그 가운데 임의의 점을 찍는다.
4) 임의의 점을 찍서 생상던 표본을 선정된 집단에 다시 포함시키고, 소수집단이 다수 집단과 숫자가 같아질 때까지 반복한다.

SMOTE는 기존에 있는 데이터를 단순 복제하는 것이 아니라 소수 범주의 데이터들을 서로 보완하여 새로운 데이터를 합성하는 방법입니다. 
이 기법은 먼저 k 근접 이웃 (k nearest neighbor) 알고리즘을 사용해 소수 범주의 데이터들과 가장 가까운 데이터들을 찾은 뒤 
새로 합성 된 데이터가 그 성향을 반영하도록 하는 오버샘플링 방식입니다. 
ADASYN 오버 샘플링 방법과 유사하게 데이터를 생성하지만 생성된 데이터를 무조건 소수 클래스라고 하지 않고 분류 모형에 따라 분류합니다.

2. ADASYN (Adaptive Synthetic Sampling Approach)
He et al.(2008)가 제안한 방법으로 SMOTE를 발전시킨 방법입니다. ADASYN 은 데이터들의 밀도 분포인 r 를 계산하여 
범주마다 다른 양의 데이터를 합성합니다.
ADASYN은 주위 데이터의 분포에 따라 오버 샘플링 할 데이터를 좀 더 체계적으로 조절하는 방법입니다.

3. ROS or ROSE (Random Over Sampling)

랜덤 오버 샘플링 방법은 데이터 수가 작은 집단의 표본 수를 늘리기 위한 방법으로 표본을 무작위로 선택하여 반복 추출하는 방법입니다.
표본의 수가 증가하지만 표본을 반복적으로 늘린 것이라 정보의 양적 측면에서는 증가했다고 보긴 어렵습니다.
오히려 중복된 표본으로 인해 Overfitting의 문제가 발생할 가능성도 증가합니다.



불균형 데이터 문제
모델을 학습시킴에 있어 클래스 간 샘플 수의 차이가 너무 크게 되면 분류기는 더 많은 샘플이 존재하는 클래스로 편향(bias, 바이어스)된다. 이 경우 전체적인 정확도는 높게 나올지라도 샘플 수가 적은 클래스에 대한 재현율(recall)이 작아지게 된다. 
(즉, 특정 클래스에 오버피팅 된다.) 
이러한 문제를 데이터 불균형 문제 혹은 비대칭 문제라고 부른다.
암환자를 분류하는 문제를 생각해보자. 암에 걸린 사람보다는 암에 걸리지 않은 사람이 더 많기 때문에 암환자로부터 얻어진 샘플이 그렇지 않은 경우의 샘플보다 훨씬 적을 수 있다.
이 경우 데이터 불균형 문제를 해결하지 않고 학습하게 되면 분류기는 정상인의 샘플에 더 많은 가중치를 두고 학습할 것이고, 
[그림 1]과 같이 분류 경계면에 바이어스가 존재하게 되어 암환자를 정상인으로 잘못 분류할 수 있게 된다. 이러한 문제는 불량품과 양품을 구분하는 경우에도 발생할 것이다.
즉, 현실 세계에서 우리가 풀고자하는 많은 문제들은 클래스 간 데이터 불균형 문제가 항상 있으며, 보통은 더 중요하게 여겨지는 클래스(예를 들면, 암환자 클래스, 불량품 클래스)의 데이터가 더 적은 것이 일반적이다.
데이터 불균형이 심한 경우 모델의 일반화 성능을 이끌어내기 어렵기 때문에 데이터 불균형 문제를 먼저 해결하고 모델을 학습시켜야 한다. 더 많은 데이터를 수집하여 이 문제를 해결하면 좋겠지만 그렇지 못할 경우, 불균형 데이터 문제를 완화할 수 있는 기법들이 무엇이 있는지 알아보자.

해결 방안
클래스 간 데이터 불균형을 해결하는 방법들은 다양한데 크게 아래 [그림 2]와 같이 나눌 수 있다.

언더 샘플링
- 다수 범주의 데이터를 줄여서 소수 범주의 샘플 수와 비슷하게 만든다.
- 다수 범주 관측치 제거로 계산 시간 감소
- 데이터 클랜징으로 클래스 오버랩 감소
- 데이터 제거로 인한 정보 손실 발생 (중요 정보가 삭제될 시 치명적)
오버 샘플링
- 소수 범주의 데이터를 증폭시켜서 다수 범주의 샘플 수와 비슷하게 만든다.
- 정보 손실이 없다.
- 보통 언더 샘플링보다 분류 정확도가 높다.
- 과적합 가능성이 있고, 계산 시간이 오래 걸리며, 노이즈나 이상치에 민감하다.

언더 샘플링 기법들
Random Undersampling
다수 범주의 샘플들을 무작위로 선택
샘플 선택에 아무런 제약이 없기 때문에 처리 시간이 빠르다.
무작위 선택으로 인해 샘플링 결과에 따라 분류 경계면의 변동이 심하다.

Tomek Link
Tomek link를 형성하는 두 샘플 중 하나는 노이즈이거나 둘 다 경계선 근처에 있다.
Tomek link를 형성한 샘플 중 다수 범주에 속한 샘플을 제거한다.
[그림 4]는 Tomek link의 예제를 보여준다. 좌측과 같은 데이터 세트에 대하여 Tomek link를 형성하면 가운데 그림과 같이 된다. Tomek link를 형성한 파란색 샘플들을 제외하면 우측 그림과 같이 되는데 샘플을 제외하기 이전보다 두 범주의 구분이 보다 명확히 됨을 알 수 있다. 따라서 두 범주 간의 분류 경계면을 찾는데 도움이 된다.
Tomek link는 다수 범주의 데이터의 중심 분포는 거의 유지하면서 분류 경계를 조정하는 효과를 얻기 때문에 random undersampling에 비해 정보의 유실을 크게 줄일 수 있지만, 제거되는 샘플이 한정적이기 때문에 큰 언더 샘플링의 효과를 얻을 수는 없다.

Condensed Nearest Neighbor (CNN)
본래 KNN 알고리즘을 사용하기 위한 목적으로 데이터 세트의 크기를 줄이기 위해 고안되었다.
다수 범주에서는 하나의 샘플을 무작위로 선택하고 동시에 소수 범주에서는 모든 샘플을 선택하여 서브 데이터를 구성한다.
원데이터를 서브 데이터를 기준으로 1-nearest neighbor(1-NN) 분류한다.
- 서브 데이터를 구성할 때, 다수 범주에서 하나의 샘플을 선택하였기 때문에 반드시 1-NN을 사용하여야 한다. (그렇다면 3개의 샘플을 선택하면 3-NN을 해도 될까??)
다수 범주에서 소수 범주로 분류된 샘플과 서브 데이터만을 남기고 나머지 샘플은 삭제한다.
위 과정을 통해 다수 범주의 데이터는 다운 샘플링 되며, 이렇게 얻어진 샘플들을 이용해 분류 경계면을 학습한다.
위 과정을 도식적으로 나타내면 아래 [그림 5]와 같다.

One-Sided Selection (OSS)
Tomek link + CNN
Tomek link로 다수 범주의 데이터를 제거 후 CNN을 사용하여 다시 한 번 언더 샘플링 수행한다.
- Tomek link로 클래스의 경계면에 있는 데이터를 제거하고 CNN으로는 경계면에서 멀리 떨어진 다수 범주의 데이터를 제거한다.
- Tomek link와 CNN의 한계를 상호 보완

오버 샘플링 기법들
Resampling
소수 범주의 데이터를 단순히 복제하여 데이터의 수를 늘린다.
단순 복제이므로 새로운 데이터를 생성하는 것은 아니다. 그렇지만 분류 경계면을 결정할 때 소수 클래스에 대한 가중치가 증가하기 때문에 성능 향상에 도움이 된다.
소수 범주의 데이터 세트가 전체 모집단을 대표한다는 보장이 없기 때문에 소수 범주에 과적합이 발생할 가능성이 있다.
아래 [그림 7]의 좌측은 원시 데이터이고 우측은 Resampling된 데이터 세트이다. 우측 그림의 숫자가 의미하는 바는 해당 샘플이 복제된 횟수이다.

Synthetic Minority Oversampling TEchnique (SMOTE)
소수 범주의 데이터를 단순 복제하는 Resampling 기법과는 다르게 소수 범주의 데이터를 가상으로 만들어 낸다.
[그림 8]의 좌측과 같은 원시 데이터 세트가 있다고 할 때, SMOTE는 다음과 같은 절차로 이루어진다.
1. 소수 범주의 데이터 중 무작위로 하나를 선택한다. 선택된 데이터를 X라 하자.
2. 무작위로 선택된 데이터를 기준으로 KNN을 수행한다. (k>1, [그림 8]에서는 k=5)
3. 선택된 K-nearest neighbor 중 하나를 임의로 선택한다. 선택된 데이터를 X(NN)이라고 하자. ([그림 8]의 우측 그림에서 보라색으로 표시)
4. 다음 수식을 통해 가상의 데이터를 생성한다. 여기서 μ은 0~1사이의 균일 분포에서 추출된 임의의 값이다.
5. 소수 범주 내 모든 데이터에 대하여 1~4과정을 반복한다.

Borderline SMOTE
범주 간의 경계부분에만 오버 샘플링을 하는 것이 분류모델을 학습시킬 때 더 도움이 될 것이라는 생각에서 제안되었다.
Danger 관측치에 대하여만 SMOTE 적용한다.
SMOTE 적용할 때 KNN 알고리즘 적용시에는 소수 관측치의 모든 데이터를 사용한다.
결과적으로 borderline 근처에만 새로운 데이터들이 생기게 된다.

ADAptive SYNthetic sampling approach (ADASYN)
Borderline SMOTE와 유사하지만 샘플링하는 개수를 위치에 따라 다르게 적용한다.
샘플링 개수를 정하는 규칙과 전체적인 알고리즘 과정은 다음과 같다. ([그림 10] 참고)
- 모든 i에 대하여 계산된 개수만큼 SMOTE 알고리즘을 적용하여 오버샘플링을 수행한다.
ADASYN은 Borderline 뿐만 아니라 다수 클래스 쪽에 있는 소수 클래스 데이터 주변에도 많은 샘플을 생성한다.
